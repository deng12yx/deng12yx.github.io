<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="theme-color" content="#123456"><meta name="generator" content="Hexo 4.2.0"><title>FGSM &amp; PDG - BeautyFlower</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Icaurs - Hexo Theme"><meta name="msapplication-TileImage" content="icons/touch-icon-iphone.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icaurs - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="144x144" href="icons/touch-icon-iphone.png"><link rel="apple-touch-icon" sizes="152x152" href="icons/touch-icon-ipad.png"><link rel="apple-touch-icon" sizes="72x72" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="96x96" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="128x128" href="icon/logo.ico"><link rel="apple-touch-icon" sizes="256x256" href="icon/logo.ico"><meta name="description" content="FGSM 论文链接： [1412.6572] Explaining and Harnessing Adversarial Examples (arxiv.org)[1412.6572] Explaining and Harnessing Adversarial Examples (arxiv.org) 研究点 设计更强大的优化方法，能够在训练过程中有效处理非线性模型的复杂性，从而同时实现"><meta property="og:type" content="blog"><meta property="og:title" content="FGSM &amp; PDG"><meta property="og:url" content="http://example.com/post/cda691ed.html"><meta property="og:site_name" content="BeautyFlower"><meta property="og:description" content="FGSM 论文链接： [1412.6572] Explaining and Harnessing Adversarial Examples (arxiv.org)[1412.6572] Explaining and Harnessing Adversarial Examples (arxiv.org) 研究点 设计更强大的优化方法，能够在训练过程中有效处理非线性模型的复杂性，从而同时实现"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/article/13.png"><meta property="article:published_time" content="2024-07-02T08:20:29.000Z"><meta property="article:modified_time" content="2024-09-28T16:17:24.000Z"><meta property="article:author" content="yyyyyyxnp"><meta property="article:tag" content="Python"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/article/13.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/post/cda691ed.html"},"headline":"FGSM & PDG","image":["http://example.com/img/article/13.png"],"datePublished":"2024-07-02T08:20:29.000Z","dateModified":"2024-09-28T16:17:24.000Z","author":{"@type":"Person","name":"yyyyyyxnp"},"publisher":{"@type":"Organization","name":"BeautyFlower","logo":{"@type":"ImageObject","url":{"text":"BeautyFlower"}}},"description":"FGSM\r 论文链接： [1412.6572]\r Explaining and Harnessing Adversarial Examples (arxiv.org)[1412.6572] Explaining and\r Harnessing Adversarial Examples (arxiv.org)\r 研究点\r 设计更强大的优化方法，能够在训练过程中有效处理非线性模型的复杂性，从而同时实现"}</script><link rel="canonical" href="http://example.com/post/cda691ed.html"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="BeautyFlower" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">BeautyFlower</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/deng12yx"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/article/13.png" alt="FGSM &amp; PDG"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item copyright article-title type-2">原创</span><span class="level-item">Posted&nbsp;<time dateTime="2024-07-02T08:20:29.000Z" title="2024/7/2 16:20:29">2024-07-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-28T16:17:24.000Z" title="2024/9/29 00:17:24">2024-09-29</time></span><span class="level-item"><a class="link-muted" href="/categories/%E5%AE%89%E5%85%A8/">安全</a></span><span class="level-item">an hour read (About 8654 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile"> FGSM &amp; PDG</h1><div class="copyright article-block type-2"><p>版权申明：本文为原创文章，转载请注明原文出处</p><p>原文链接：<a href="http://example.com/post/cda691ed.html" target="_blank">http://example.com/post/cda691ed.html</a></p></div><h1 class="title is-3 is-size-4-mobile">FGSM &amp; PDG</h1><div class="content"><h2 id="fgsm">FGSM</h2>
<p>论文链接： [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6572">1412.6572]
Explaining and Harnessing Adversarial Examples (arxiv.org)</a>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6572">1412.6572] Explaining and
Harnessing Adversarial Examples (arxiv.org)</a></p>
<h4 id="研究点">研究点</h4>
<p>设计更强大的优化方法，能够在训练过程中有效处理非线性模型的复杂性，从而同时实现训练的易用性和对抗性扰动的抵抗能力</p>
<span id="more"></span>
<h4 id="线性模型中的对抗扰动">线性模型中的对抗扰动</h4>
<ol type="1">
<li><strong>对抗扰动的累积效应</strong>：
<ul>
<li>假设我们有一个输入向量 <span class="math inline">\(x\)</span>
和一个权重向量 <span class="math inline">\(w\)</span>。</li>
<li>我们施加一个微小的扰动 <span class="math inline">\(η\)</span>
到输入向量 <span class="math inline">\(x\)</span>，得到新的输入 <span class="math inline">\(\tilde{x}=x+\eta.\)</span>。</li>
</ul></li>
<li><strong>权重向量和扰动的点积</strong>：
<ul>
<li>对于线性模型，激活值是通过计算权重向量 <span class="math inline">\(w\)</span> 和输入向量 <span class="math inline">\(x\)</span> 的点积得到的，即<span class="math inline">\(\boldsymbol{w}^\top\tilde{\boldsymbol{x}}=\boldsymbol{w}^\top\boldsymbol{x}+\boldsymbol{w}^\top\boldsymbol{\eta}\)</span>。</li>
<li>这里 <span class="math inline">\(w^\top
x\)</span>是原始激活值，而<span class="math inline">\(w^\top
n\)</span>是由扰动引起的激活值变化。</li>
</ul></li>
<li><strong>最大化扰动的影响</strong>：
<ul>
<li>为了使激活值变化<span class="math inline">\(w^\top
n\)</span>最大化，可以将 <span class="math inline">\(η\)</span> 设置为
<span class="math inline">\(\mathrm{sign}(w)\)</span>，即 <span class="math inline">\(η\)</span> 的每个元素的符号与 <span class="math inline">\(w\)</span> 的对应元素相同。</li>
<li>这种设置会使得点积 <span class="math inline">\(w^\top n\)</span>
取得最大值。</li>
</ul></li>
<li><strong>累积效应</strong>：
<ul>
<li>如果权重向量 <span class="math inline">\(w\)</span> 有 <span class="math inline">\(n\)</span> 个维度，每个元素的平均值为 <span class="math inline">\(m\)</span>，那么总的激活值变化为<span class="math inline">\(\epsilon mn\)</span>。</li>
<li>这是因为每个小的扰动 <span class="math inline">\(n\)</span>
的元素都被放大了 <span class="math inline">\(m\)</span> 倍，并且有 <span class="math inline">\(n\)</span> 个这样的元素。</li>
</ul></li>
<li><strong>高维度的影响</strong>：
<ul>
<li>由于 <span class="math inline">\(η\)</span> 的最大范数<span class="math inline">\(\|\eta\|_{\infty}\)</span>是固定的（即每个元素的最大值是固定的），不会随着维度<span class="math inline">\(n\)</span>增加而增加。</li>
<li>但是，由于<span class="math inline">\(n\)</span>增加，累积效应使得总的激活值变化<span class="math inline">\(w^\top n\)</span>可以线性增加。</li>
</ul></li>
<li><strong>意外隐写术</strong>：
<ul>
<li>这种现象类似于隐写术，即通过许多小的变化隐藏一个大的变化。</li>
<li>线性模型会被迫专注于最符合其权重的信号，即使输入数据中存在多个信号且其他信号的幅度更大。</li>
</ul></li>
</ol>
<h4 id="非线性模型的线性特性">非线性模型的线性特性</h4>
<p>非线性模型被有意设计为以非常线性的方式运行</p>
<h5 id="lstm长短期记忆网络">LSTM（长短期记忆网络）</h5>
<h6 id="公式和结构">公式和结构：</h6>
<p>LSTM单元由以下主要部分组成：</p>
<ul>
<li><strong>输入门</strong>：控制有多少新信息被写入记忆单元。</li>
<li><strong>遗忘门</strong>：控制有多少记忆单元中的信息被保留。</li>
<li><strong>输出门</strong>：控制有多少信息从记忆单元输出。</li>
</ul>
<p>LSTM单元的公式如下：</p>
<p><span class="math inline">\(\begin{aligned}
&amp;i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i) \\
&amp;f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f) \\
&amp;o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o) \\
&amp;\tilde{C}_t=\tanh(W_C\cdot[h_{t-1},x_t]+b_C) \\
&amp;C_t=f_t\cdot C_{t-1}+i_t\cdot\tilde{C}_t \\
&amp;h_t=o_t\cdot\tanh(C_t)
\end{aligned}\)</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(σ\)</span> 是sigmoid函数，输出范围在(0,
1)之间。</li>
<li><span class="math inline">\(⁡tanh\)</span> 是tanh函数，输出范围在(-1,
1)之间。</li>
<li><span class="math inline">\(i_t\)</span>、<span class="math inline">\(f_t\)</span>、<span class="math inline">\(o_t\)</span>
分别是输入门、遗忘门和输出门的激活值。</li>
<li><span class="math inline">\(C_t\)</span> 是记忆单元的状态。</li>
<li><span class="math inline">\(h_t\)</span> 是隐藏状态。</li>
</ul>
<h6 id="线性特性">线性特性：</h6>
<ul>
<li><strong>门控机制</strong>：LSTM通过输入门、遗忘门和输出门控制信息的流动，这些门的输出是线性的（在0到1之间）。这种设计使得信息在较长时间内保持稳定，梯度传递时不会消失或爆炸，从而表现得更为线性。</li>
</ul>
<h5 id="relu整流线性单元">ReLU（整流线性单元）</h5>
<h6 id="公式和特性">公式和特性：</h6>
<p>ReLU的激活函数定义为： <span class="math inline">\(𝑓(𝑥)=max⁡(0,𝑥)\)</span></p>
<h6 id="线性特性-1">线性特性：</h6>
<ul>
<li><strong>线性部分</strong>：当输入 <span class="math inline">\(𝑥≥0\)</span> 时，ReLU的输出是 <span class="math inline">\(x\)</span>，即线性关系；当输入 <span class="math inline">\(𝑥&lt;0\)</span>时，输出为0。这使得在大部分情况下，ReLU的输出是线性的，尤其是在正区间内。</li>
<li><strong>避免梯度消失</strong>：与sigmoid和tanh等激活函数相比，ReLU的正区间内梯度为1，不会出现梯度消失的问题，使得优化过程更加高效。</li>
</ul>
<h5 id="maxout网络">Maxout网络</h5>
<h6 id="公式和特性-1">公式和特性：</h6>
<p>Maxout单元输出若干线性函数的最大值，具体公式为：</p>
<p><span class="math inline">\(f(x)=\max(w_1^\top x+b_1,w_2^\top
x+b_2,\ldots,w_k^\top x+b_k)\)</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(w_i\)</span>和 <span class="math inline">\(b_i\)</span> 是可学习的参数。</li>
<li><span class="math inline">\(k\)</span>
是Maxout单元的线性函数数量。</li>
</ul>
<h6 id="线性特性-2">线性特性：</h6>
<ul>
<li><strong>线性组合</strong>：Maxout通过输出多个线性函数中的最大值，实现在局部线性化的非线性映射。这种设计使得每个Maxout单元在一小段范围内是线性的。</li>
<li><strong>增强非线性表达能力</strong>：虽然Maxout单元在局部是线性的，但通过组合多个线性函数，整体上可以实现复杂的非线性映射，同时保持优化的稳定性。</li>
</ul>
<h4 id="初始fgsm">初始FGSM</h4>
<p>快速梯度符号法 —— 生成对抗样本</p>
<ol type="1">
<li><strong>符号说明</strong>：
<ul>
<li><span class="math inline">\(θ\)</span>：模型的参数。</li>
<li><span class="math inline">\(𝑥\)</span>：模型的输入。</li>
<li><span class="math inline">\(𝑦\)</span>：与 <span class="math inline">\(𝑥\)</span>
相关联的目标（对于有目标的机器学习任务）。</li>
<li><span class="math inline">\(𝐽(𝜃,𝑥,𝑦)\)</span>：用于训练神经网络的损失函数（成本函数）。</li>
</ul></li>
<li><strong>线性化损失函数</strong>：
<ul>
<li>我们可以在当前参数 <span class="math inline">\(𝜃\)</span>
的值附近对损失函数 <span class="math inline">\(𝐽(𝜃,𝑥,𝑦)\)</span>
进行线性化处理，以获得一个最佳的最大范数约束扰动。</li>
</ul></li>
<li><strong>生成对抗扰动</strong>：
<ul>
<li>对抗扰动 <span class="math inline">\(𝜂\)</span> 可以表示为： <span class="math inline">\(𝜂=𝜖 *
\mathrm{sign}(\nabla_xJ(\theta,x,y))\)</span></li>
<li>其中，<span class="math inline">\(𝜖\)</span>
是一个小的常数，用于控制扰动的大小。</li>
<li><span class="math inline">\(\nabla_xJ(\theta,x,y)\)</span>
表示损失函数对输入 <span class="math inline">\(𝑥\)</span> 的梯度。</li>
<li>sign(⋅)sign(⋅) 是符号函数，表示取梯度中每个元素的符号（正为 +1，负为
-1）。</li>
</ul></li>
<li><strong>快速梯度符号法（FGSM）</strong>：
<ul>
<li>这种方法被称为快速梯度符号法，用于生成对抗样本。</li>
<li>通过反向传播算法，可以高效地计算所需的梯度。</li>
</ul></li>
</ol>
<h5 id="具体实例">具体实例</h5>
<p>假设有一个简单的神经网络模型用于图像分类，输入 <span class="math inline">\(𝑥\)</span> 是一个手写数字图片，目标 <span class="math inline">\(y\)</span>是图片对应的数字（比如3）。</p>
<ol type="1">
<li><strong>模型参数和损失函数</strong>：
<ul>
<li>模型的参数 <span class="math inline">\(θ\)</span>
包括权重和偏置。</li>
<li>损失函数 <span class="math inline">\(𝐽(𝜃,𝑥,𝑦)\)</span>是交叉熵损失函数，用于评估模型预测的准确性。</li>
</ul></li>
<li><strong>计算损失函数对输入的梯度</strong>：
<ul>
<li>通过反向传播算法，我们可以计算损失函数$ 𝐽(𝜃,𝑥,𝑦)$ 对输入 <span class="math inline">\(𝑥\)</span> 的梯度<span class="math inline">\(\nabla_xJ(\theta,x,y)\)</span>。</li>
</ul></li>
<li><strong>生成对抗扰动</strong>：
<ul>
<li>假设我们计算出的梯度为：<span class="math inline">\(\nabla_xJ(\theta,x,y)=\begin{bmatrix}-0.1&amp;0.3&amp;-0.2&amp;0.4\end{bmatrix}\)</span></li>
<li>使用符号函数得到： <span class="math inline">\(\mathrm{sign}(\nabla_xJ(\theta,x,y))=\begin{bmatrix}-1&amp;1&amp;-1&amp;1\end{bmatrix}\)</span></li>
<li>设定一个小的扰动幅度<span class="math inline">\(\epsilon=0.1\)</span>，我们得到对抗扰动：<span class="math inline">\(\eta=0.1\cdot\begin{bmatrix}-1&amp;1&amp;-1&amp;1\end{bmatrix}=\begin{bmatrix}-0.1&amp;0.1&amp;-0.1&amp;0.1\end{bmatrix}\)</span></li>
</ul></li>
<li><strong>生成对抗样本</strong>：
<ul>
<li>我们将扰动添加到原始输入 <span class="math inline">\(𝑥\)</span>
中，得到对抗样本<span class="math inline">\(\tilde{x}\)</span>：
$=x+$</li>
</ul></li>
<li><strong>对抗样本的影响</strong>：
<ul>
<li>新的对抗样本<span class="math inline">\(\tilde{x}\)</span>
看起来与原始输入<span class="math inline">\(x\)</span>很相似，但由于添加了精心设计的扰动，神经网络可能会错误地分类这个对抗样本。</li>
</ul></li>
</ol>
<h4 id="l1正则化-vs-fgsm对抗性训练">L1正则化 vs FGSM对抗性训练</h4>
<h5 id="l1正则化">L1正则化</h5>
<ul>
<li>L1正则化通过在损失函数中加入权重的 L1
范数（即权重的绝对值和）来控制模型的复杂度，从而避免过拟合。</li>
<li>损失函数为<span class="math inline">\(\mathcal{L}(w,b)+\lambda||w||_1\)</span>，其中<span class="math inline">\(\lambda\)</span>是正则化参数。通过最小化这个损失函数，模型不仅需要减少原始损失（提高准确率），还需要控制权重的绝对值之和。这会使得某些权重变得更小，甚至接近于零，从而达到减少模型复杂度、防止过拟合的目的。</li>
</ul>
<h5 id="对抗性训练">对抗性训练</h5>
<ul>
<li><p>在对抗性训练中，我们最小化的损失函数为<span class="math inline">\(\mathbb{E}_{x,y\sim
p_{\mathrm{data}}}\zeta(y(||w||_1-w^Tx-b))\)</span>。</p></li>
<li><p>这里的损失函数在训练过程中直接减去了权重的 L1
范数，而不是将其加到损失函数中。这种减法会在模型信心足够高时（即$$函数饱和时）使惩罚项逐渐消失。</p>
<blockquote>
<p>这里所说的饱和指的是当模型对某个样本的预测非常自信时，<span class="math inline">\(w^Tx+b\)</span>的值使得$<span class="math inline">\(函数的输入值变得非常小，无论是正类还是负类样本，这导致\)</span><span class="math inline">\(函数的值也很小，从而使得对抗性训练中的损失变得非常小。这种情况下，惩罚项\)</span>||w||_1$对总损失的影响变得不明显，因为模型在自信的预测上已经让损失趋近于零。</p>
</blockquote></li>
</ul>
<h5 id="总结">总结</h5>
<ul>
<li><strong>L1
正则化</strong>通过惩罚权重大小来控制模型复杂度，始终存在于损失函数中。在训练过程中将
L1 惩罚项加到损失函数上，即使损失小，惩罚还是一样会增加</li>
<li><strong>对抗性训练</strong>通过扰动输入数据增强模型的鲁棒性，损失函数中的惩罚项会在模型足够自信时消失。对抗性训练则将其减去，损失越小，对抗性训练的惩罚越少，达到饱和，不再影响（当模型欠拟合时，对抗性训练会导致进一步欠拟合）</li>
</ul>
<h4 id="深度模型的对抗扰动">深度模型的对抗扰动</h4>
<blockquote>
<p>通用逼近定理：给定足够数量的隐藏单元和适当的权重，一个神经网络可以表示从输入到输出的几乎任何复杂关系。</p>
</blockquote>
<h5 id="对抗性训练-vs-数据增强">对抗性训练 vs 数据增强</h5>
<p>前者通过对抗样本直接暴露模型的弱点，使得模型在训练过程中不断改进其决策边界，变得更加鲁棒。</p>
<p>后者虽然能提供多样化的训练数据，但对模型潜在的脆弱性没有直接的改进效果。</p>
<h5 id="损失函数优化">损失函数优化</h5>
<p><span class="math inline">\(\tilde{J}(\boldsymbol{\theta},\boldsymbol{x},y)=\alpha
J(\boldsymbol{\theta},\boldsymbol{x},y)+(1-\alpha)J(\boldsymbol{\theta},\boldsymbol{x}+\epsilon
\,\mathrm{sign}\left(\nabla_{\boldsymbol{x}}J(\boldsymbol{\theta},\boldsymbol{x},y)\right).\)</span></p>
<p>前半部分是模型在标准输入 <span class="math inline">\(\boldsymbol{x}\)</span>
上的损失，通常用于衡量模型在干净数据上的表现</p>
<p>后半部分是模型在对抗样本 <span class="math inline">\(\boldsymbol{x} +
\epsilon \, \mathrm{sign}\left(\nabla_{\boldsymbol{x}}
J(\boldsymbol{\theta}, \boldsymbol{x}, y)\right)\)</span>
上的损失。对抗样本是通过对标准输入 <span class="math inline">\(\boldsymbol{x}\)</span>
进行小扰动生成的，这种扰动方向由损失函数对输入的梯度决定，目的是最大化损失。</p>
<p>α 控制标准损失和对抗损失的相对权重。当 <span class="math inline">\(\alpha\)</span> 接近 1 时，更加关注标准损失；当
<span class="math inline">\(\alpha\)</span> 接近 0
时，更加关注对抗损失。</p>
<h5 id="噪声添加位置选择">噪声添加位置选择</h5>
<ul>
<li>Szegedy等人的实验表明，在一个使用sigmoid激活函数的神经网络中，如果在隐藏层添加噪声，模型的正则化效果（即防止过拟合的能力）最好。</li>
<li>在使用快速梯度符号方法的实验中，研究人员发现，对于那些隐藏单元激活值无界的网络，隐藏单元的激活值会变得非常大。因此，通常最好只是扰动原始输入。</li>
<li>在饱和模型（如Rust模型）中，研究人员发现扰动输入层与扰动隐藏层的效果相当。</li>
<li>基于旋转隐藏层的扰动解决了无界激活值增长的问题，使得加性扰动相对较小。研究人员成功训练了带有隐藏层旋转扰动的maxout网络，但效果不如扰动输入层强。</li>
<li>由于神经网络的最后一层（线性- sigmoid或线性-
softmax层）不是最终隐藏层函数的通用逼近器，将对抗扰动应用于最后隐藏层可能会遇到<strong>欠拟合</strong>问题。研究人员确实发现了这种效果。</li>
</ul>
<p>研究人员认为，对抗训练只有在模型<strong>有能力学习抵抗对抗样本时</strong>才明显有用。这仅在通用近似定理适用时才明显。</p>
<h5 id="容量低的模型表现出好性能rbf">容量低的模型表现出好性能（RBF）</h5>
<blockquote>
<p><strong>RBF（径向基函数）</strong>：一种常用的激活函数，通常用于神经网络中。RBF网络使用这些函数来将输入映射到高维空间，并进行分类或回归。假设我们有一个RBF网络用于分类任务，它的激活函数是高斯函数，表示为：
<span class="math inline">\(\phi(x) = \exp(-\frac{\|x -
\mu\|^2}{2\sigma^2})\)</span> 其中，<span class="math inline">\(\mu\)</span> 是中心，<span class="math inline">\(\sigma\)</span>
是标准差。RBF网络会根据输入与中心的距离来计算激活值。</p>
</blockquote>
<p>由于RBF单元只对特定点有强响应，对其他点的响应较弱或没有响应，这意味着它们无法识别所有相关的正类样本，从而降低了召回率。假设RBF网络用于识别手写数字“3”。网络可能只对一些非常典型的“3”有强烈响应，但对一些稍微不同的“3”响应较弱，导致它们未被识别。</p>
<p>由于RBF单元在对抗样本（即故意加入扰动以欺骗模型的样本）上的表现较好，因为它们在不确定的情况下置信度较低，研究人员希望通过二次单元模型进一步提高这种鲁棒性。这类模型包括了更复杂的激活函数（例如二次函数），以更好地处理对抗样本。</p>
<p>在尝试使用具有足够二次抑制（即能够抵抗对抗扰动的能力）的模型时，研究人员发现这些模型在训练集上的误差很高。这意味着模型虽然在理论上能够抵抗对抗扰动，但在实际训练中表现不佳。</p>
<h5 id="对抗样本在不同模型之间的泛化现象">对抗样本在不同模型之间的泛化现象</h5>
<p>对抗样本是故意添加微小扰动，使模型做出错误预测的样本。一个为某个模型生成的对抗样本，往往也能欺骗其他模型，即使这些模型的结构不同或在不同的数据上训练过。</p>
<p>我们可以用“线性视角”来解释这一现象。根据这种观点，对抗样本在高维空间中形成了广泛的区域，而不是精细的点。</p>
<p>具体来说，只要扰动的方向和模型的损失函数梯度方向一致，并且扰动幅度足够大，就能生成对抗样本。这些样本并不是分散在精确的位置上，而是覆盖了一个较大的子空间。</p>
<p>研究人员假设，当前的训练方法使得神经网络在面对对抗样本时，表现得类似于一个简单的线性分类器。即使这些模型结构不同，它们学到的分类权重是相似的，这导致了对抗样本的跨模型泛化现象。</p>
<p>这个假设解释了一部分模型错误分类的原因，但并不能解释所有情况。</p>
<h2 id="pgd">PGD</h2>
<blockquote>
<p>投影梯度下降（PGD）作为通用的“一阶对手”，即利用有关网络的本地一阶信息的最强攻击。</p>
</blockquote>
<h4 id="研究点-1">研究点</h4>
<p>精确地了解我们想要实现的安全保证类型，即我们想要抵抗的广泛类型的攻击（与仅防御特定的已知攻击相反）</p>
<p>将攻击和防御纳入一个共同的理论框架中，将对抗性训练直接对应于优化</p>
<h4 id="pgd方法">PGD方法</h4>
<ol type="1">
<li><p>攻击目标：PGD的目标是生成对抗性样本，即在原始输入数据 <span class="math inline">\(x\)</span> 上添加扰动 <span class="math inline">\(\delta\)</span>，使得最大化模型的损失函数 <span class="math inline">\(L(\theta, x + \delta,
y)\)</span>。这个损失函数考虑了在给定模型参数 <span class="math inline">\(\theta\)</span> 下，经过扰动后的输入<span class="math inline">\(x + \delta\)</span>
的预测误差。攻击者的目标是找到一个使得损失函数值最大化的扰动 <span class="math inline">\(\delta\)</span>。</p></li>
<li><p>扰动的约束：PGD通常会限制扰动 <span class="math inline">\(\delta\)</span>
的大小，以确保生成的对抗性样本在人类感知上仍然是接近原始样本的。这种限制可以是
<span class="math inline">\(\ell_p\)</span> 范数约束（如 <span class="math inline">\(\ell_\infty\)</span>
范数，即扰动的每个分量都不超过某个最大值），也可以是其他形式的约束，例如在像素空间中保持每个像素值在合理范围内。</p></li>
<li><p>算法步骤：</p>
<ol type="1">
<li><strong>初始化</strong>：从原始输入 <span class="math inline">\(x\)</span> 开始，设定一个初始扰动 <span class="math inline">\(\delta^{(0)}\)</span>。</li>
<li><strong>迭代优化</strong>：
<ul>
<li>对于每一轮 <span class="math inline">\(t\)</span>，计算梯度 <span class="math inline">\(\nabla_\delta L(\theta, x + \delta^{(t)},
y)\)</span>。</li>
<li>将扰动 <span class="math inline">\(\delta^{(t)}\)</span> 更新为
<span class="math inline">\(\delta^{(t+1)} =
\text{Clip}_{\epsilon}(\delta^{(t)} + \alpha \cdot
\text{sign}(\nabla_\delta L(\theta, x + \delta^{(t)},
y)))\)</span>，其中 <span class="math inline">\(\alpha\)</span>
是学习率，<span class="math inline">\(\text{Clip}_{\epsilon}\)</span>
表示对扰动进行约束，确保其不超过 <span class="math inline">\(\epsilon\)</span> 的范围。</li>
</ul></li>
<li><strong>终止条件</strong>：通常根据迭代次数或者达到满足某个条件的对抗性样本来决定停止迭代。</li>
</ol></li>
<li><p>优化方法：PGD通过迭代优化来找到最大化损失函数的扰动 <span class="math inline">\(\delta\)</span>，同时尽可能维持扰动的大小在合理范围内，以确保生成的对抗性样本在实际应用中具有可行性和现实意义。</p></li>
<li><p>结论：</p>
<ul>
<li>使用投影梯度下降（PGD）生成对抗性样本时，对抗性损失（adversarial
loss）会以相当一致的方式快速增加</li>
<li>通过PGD优化生成的对抗性样本的损失值在多次尝试后趋于一致。这种现象暗示着损失函数的优化空间可能存在局部最大值，但这些最大值的分布是相对稳定和可预测的。</li>
<li>损失函数的优化空间具有一定的结构性质，即在对抗性攻击中，通过微小扰动生成的对抗性样本的损失函数是相对一致且可预测的。这也暗示了在给定条件下，攻击者可以利用这种结构来有效地生成对抗性样本。</li>
<li>在不同尺度的对抗性攻击中，对抗性样本的生成方式可能不仅仅依赖于梯度的正向调整，还可能涉及到更复杂的攻击策略和模型响应方式。这提示我们在设计防御策略时需要考虑更广泛和复杂的攻击场景。</li>
</ul>
<p>投影梯度下降（PGD）在所有一阶方法中可以被视为一种“通用”的对抗者。换句话说，通过观察和实验发现，PGD在对抗性攻击中表现出的效果和稳定性，使其成为所有基于一阶信息（例如梯度）的攻击方法中的一个普适选择。</p></li>
</ol>
<h2 id="原理总结">原理总结</h2>
<h5 id="快速梯度符号方法fgsm">1. 快速梯度符号方法（FGSM）：</h5>
<ul>
<li><strong>输入梯度计算</strong>：对于给定的输入样本 <span class="math inline">\(x\)</span>，计算其关于损失函数 <span class="math inline">\(L\)</span> 的梯度 <span class="math inline">\(\nabla_x L(\theta, x, y)\)</span>，其中 <span class="math inline">\(\theta\)</span> 是模型的参数， <span class="math inline">\(y\)</span> 是真实标签。</li>
<li><strong>对抗样本生成</strong>：根据梯度的方向和符号，对输入 <span class="math inline">\(x\)</span> 进行修改： <span class="math inline">\(x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x
L(\theta, x, y))\)</span> 其中， <span class="math inline">\(\epsilon\)</span> 是扰动的大小或步长， <span class="math inline">\(\text{sign}(\cdot)\)</span>
表示取梯度的符号。</li>
</ul>
<p>FGSM生成的对抗样本只进行一次梯度方向的扰动，因此它相对简单和高效。但由于只考虑了单步的梯度信息，生成的对抗样本可能不够健壮，容易被模型在更复杂的攻击检测中发现和修正。</p>
<h5 id="投影梯度下降pgd">2. 投影梯度下降（PGD）：</h5>
<p>PGD通过多次迭代梯度下降来生成对抗样本，以增加攻击的成功率和对抗的鲁棒性。</p>
<ul>
<li><strong>多次迭代</strong>：从原始样本 <span class="math inline">\(x\)</span> 开始，进行多轮迭代来生成对抗样本。</li>
<li><strong>梯度计算和投影</strong>：在每一轮迭代中，计算当前样本 <span class="math inline">\(x_t\)</span> 的梯度 <span class="math inline">\(\nabla_{x_t} L(\theta, x_t,
y)\)</span>，然后将梯度方向投影到一个允许的扰动集合 <span class="math inline">\(S\)</span> 中。例如，<span class="math inline">\(S\)</span> 可以是<span class="math inline">\(\epsilon\)</span>-范数球 $ |_{} $。</li>
<li><strong>对抗样本更新</strong>：根据投影后的梯度方向，更新当前样本
<span class="math inline">\(x_t\)</span>： <span class="math inline">\(x_{t+1} = \text{Clip}_{x, \epsilon}(x_t + \alpha
\cdot \text{sign}(\nabla_{x_t} L(\theta, x_t, y)))\)</span> 其中， <span class="math inline">\(\alpha\)</span> 是学习率（步长）， <span class="math inline">\(\text{Clip}_{x, \epsilon}(\cdot)\)</span>
是将结果限制在 <span class="math inline">\(x \pm \epsilon\)</span>
范围内的操作。</li>
</ul>
<p>PGD通过多次迭代和投影操作，能够更有效地探索对抗样本的空间，提高了攻击的成功率和对抗的鲁棒性。相比于FGSM，PGD生成的对抗样本更具挑战性，更难以被模型检测和防御。</p>
<h5 id="区别总结">区别总结：</h5>
<ul>
<li><strong>迭代次数</strong>：FGSM只进行一次梯度方向的修改，而PGD通过多次迭代来逐步优化对抗样本。</li>
<li><strong>复杂度</strong>：PGD比FGSM更复杂，因为它涉及到多次梯度计算和更新操作。</li>
<li><strong>对抗鲁棒性</strong>：PGD生成的对抗样本通常更具挑战性和鲁棒性，相比之下，FGSM生成的对抗样本可能较容易被检测和防御。</li>
</ul>
<h2 id="实验部分">实验部分</h2>
<h4 id="实验环境和基本参数">实验环境和基本参数</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">实验环境：</span></span><br><span class="line"><span class="string">镜像：PyTorch 1.10.0 + Python 3.8 (Ubuntu 20.04) + Cuda 11.3</span></span><br><span class="line"><span class="string">硬件配置：GPU: 1 x RTX 2080 Ti (11GB) + CPU: 12 vCPU Intel(R) Xeon(R) Platinum 8255C @ 2.50GHz</span></span><br><span class="line"><span class="string">内存: 40GB</span></span><br><span class="line"><span class="string">存储：系统盘: 30GB + 数据盘: 50GB</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()	<span class="comment"># 损失函数</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)	<span class="comment"># 优化函数</span></span><br><span class="line">epoch = <span class="number">10</span>	<span class="comment"># 训练迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../data_row&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../data_row&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>模型训练迭代次数：10次</p>
<h4 id="模型选择">模型选择</h4>
<h5 id="简单全连接神经网络">简单全连接神经网络</h5>
<p>模型结构</p>
<figure>
<img src="/post/cda691ed/SimpleFNN.png" alt="SimpleFNN">
<figcaption aria-hidden="true">SimpleFNN</figcaption>
</figure>
<h6 id="训练效果">训练效果</h6>
<p>准确率 &amp;&amp; 损失</p>
<p>全连接层的准确率达到93%左右，损失变化波动较大（可能是迭代次数太小了？）</p>
<p><img src="/post/cda691ed/SimpleFCN_accuracies.jpg" alt="SimpleFCN_accuracies" style="zoom: 20%;"><img src="/post/cda691ed/SimpleFCN_losses.jpg" alt="SimpleFCN_losses" style="zoom:20%;"></p>
<h6 id="fgsm攻击效果">FGSM攻击效果</h6>
<p>样本展示</p>
<p><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.05_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.05_batch_0_image_1" style="zoom: 33%;"><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.1_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.1_batch_0_image_1" style="zoom: 33%;"><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.15_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.15_batch_0_image_1" style="zoom: 33%;"></p>
<p><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.2_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.2_batch_0_image_1" style="zoom: 33%;"><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.25_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.25_batch_0_image_1" style="zoom: 33%;"><img src="/post/cda691ed/SimpleFCN_fgsm_epsilon_0.3_batch_0_image_1.png" alt="SimpleFCN_fgsm_epsilon_0.3_batch_0_image_1" style="zoom: 33%;"></p>
<p>攻击后的准确率随着设置的epsilons增大逐步下降，前期下降很快，特别是在0.1处下降最快，且SimpleFNN的准确度再epsilons达到0.2接近0，再增加eps就准确率就直接降为0了<img src="/post/cda691ed/SimpleFCN_fgsm_accuracies.jpg" alt="SimpleFCN_fgsm_accuracies" style="zoom:25%;"></p>
<h6 id="对抗训练之后的攻击效果"><strong>对抗训练之后的攻击效果</strong></h6>
<p>当以eps=0.1的对抗样本对模型做对抗训练，同样以eps为0.1的对抗样本做测试，能看出准确率有一个稳定上升，右边是对抗不同eps产生的对抗样本的效果</p>
<p><img src="/post/cda691ed/SimpleFCN_adversarial_fgsm_accuracies.jpg" alt="SimpleFCN_adversarial_fgsm_accuracies" style="zoom: 20%;"><img src="/post/cda691ed/SimpleFCN_adversarial_accuracies.jpg" alt="SimpleFCN_adversarial_accuracies" style="zoom:20%;"></p>
<h6 id="pgd攻击效果">PGD攻击效果</h6>
<p>自变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eps：epsilon，对抗攻击的幅度，即每次攻击时允许的最大扰动量。</span><br><span class="line">num_iter:轮数，对抗攻击迭代的次数，即进行对抗攻击的循环次数。</span><br><span class="line">alpha：单步攻击步长，每次对原始样本进行梯度上升的步长大小</span><br></pre></td></tr></table></figure>
<p>每种攻击幅度下的准确率的变化幅度并不大，eps为0.05时，变化规模为0.7480-0.7472，在eps为0.1就降到了0.3，0.15降到了0.12，后面还有进一步下降，但无论哪种攻击幅度，alpha和num_iter只要不是最小值，准确率下降效果都很好</p>
<p><img src="/post/cda691ed/SimpleFCN_0.05_accuracies.jpg" alt="SimpleFCN_0.05_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleFCN_0.1_accuracies.jpg" alt="SimpleFCN_0.1_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleFCN_0.15_accuracies.jpg" alt="SimpleFCN_0.15_accuracies" style="zoom:10%;"></p>
<p><img src="/post/cda691ed/SimpleFCN_0.2_accuracies.jpg" alt="SimpleFCN_0.2_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleFCN_0.25_accuracies.jpg" alt="SimpleFCN_0.25_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleFCN_0.3_accuracies.jpg" alt="SimpleFCN_0.3_accuracies" style="zoom:10%;"></p>
<h5 id="卷积神经网络">卷积神经网络</h5>
<p>模型结构</p>
<figure>
<img src="/post/cda691ed/SimpleCNN.png" alt="SimpleCNN">
<figcaption aria-hidden="true">SimpleCNN</figcaption>
</figure>
<h6 id="训练效果-1">训练效果</h6>
<p>准确率 &amp;&amp; 损失</p>
<p>简单卷积网络的的准确率比全连接层效果较好，且迭代一次准确率就达到91%以上，同样loss的变化存在一定波动，不够平滑，但相对于全连接层效果更好</p>
<p><img src="/post/cda691ed/SimpleCNN_accuracies.jpg" alt="SimpleCNN_accuracies" style="zoom:20%;"><img src="/post/cda691ed/SimpleCNN_losses.jpg" alt="SimpleCNN_losses" style="zoom:20%;"></p>
<h6 id="fgsm攻击效果-1">FGSM攻击效果</h6>
<p>样本展示</p>
<p><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.05_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.05_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.1_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.1_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.15_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.15_batch_0_image_1" style="zoom:33%;"></p>
<p><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.2_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.2_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.25_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.25_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/SimpleCNN_fgsm_epsilon_0.3_batch_0_image_1.png" alt="SimpleCNN_fgsm_epsilon_0.3_batch_0_image_1" style="zoom:33%;"></p>
<p>攻击后的准确率随着设置的epsilons变化</p>
<p>这个变化趋势和全连接层的变化规律相似，但对抗效果比SImpleFNN好，在eps达到0.2是准确率还没完全下降到0，同样在0.1-0.15区间下降最快</p>
<p><img src="/post/cda691ed/SimpleCNN_fgsm_accuracies.jpg" alt="SimpleCNN_fgsm_accuracies" style="zoom:25%;"></p>
<h6 id="对抗训练之后的攻击效果-1"><strong>对抗训练之后的攻击效果</strong></h6>
<p>当以eps=0.1的对抗样本对模型做对抗训练，同样以eps为0.1的对抗样本做测试，能看出准确率有一个稳定上升，右边是对抗不同eps产生的对抗样本的效果</p>
<p><img src="/post/cda691ed/SimpleCNN_adversarial_fgsm_accuracies.jpg" alt="SimpleCNN_adversarial_fgsm_accuracies" style="zoom: 20%;"><img src="/post/cda691ed/SimpleCNN_adversarial_accuracies.jpg" alt="SimpleCNN_adversarial_accuracies" style="zoom:20%;"></p>
<h6 id="pgd攻击效果-1">PGD攻击效果</h6>
<p>自变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eps：epsilon，对抗攻击的幅度，即每次攻击时允许的最大扰动量。</span><br><span class="line">num_iter:轮数，对抗攻击迭代的次数，即进行对抗攻击的循环次数。</span><br><span class="line">alpha：单步攻击步长，每次对原始样本进行梯度上升的步长大小</span><br></pre></td></tr></table></figure>
<p>在eps上升到0.15之前，攻击效果并没呈现出随着其他参数增加而变化的效果，但是相同的是，每种幅度下的攻击效果相似，准确率变化幅度不大，eps为0.05，几乎没有攻击效果，其他参数增加，攻击效果反而下降，eps为1效果同样如此，特别是alpha参数增加，攻击效果下降，而eps加到0.15及以上之后，准确率的变化形式几乎和SImpleFNN一致，但是准确率保持在了0.2的最高值，效果比全连接层好</p>
<p><img src="/post/cda691ed/SimpleCNN_0.05_accuracies.jpg" alt="SimpleCNN_0.05_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleCNN_0.1_accuracies.jpg" alt="SimpleCNN_0.1_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleCNN_0.15_accuracies.jpg" alt="SimpleCNN_0.15_accuracies" style="zoom:10%;"></p>
<p><img src="/post/cda691ed/SimpleCNN_0.2_accuracies.jpg" alt="SimpleCNN_0.2_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleCNN_0.25_accuracies.jpg" alt="SimpleCNN_0.25_accuracies" style="zoom:10%;"><img src="/post/cda691ed/SimpleCNN_0.3_accuracies.jpg" alt="SimpleCNN_0.3_accuracies" style="zoom:10%;"></p>
<h5 id="深度卷积神经网络">深度卷积神经网络</h5>
<p>模型结构</p>
<figure>
<img src="/post/cda691ed/DeepCNN.png" alt="DeepCNN">
<figcaption aria-hidden="true">DeepCNN</figcaption>
</figure>
<h6 id="训练效果-2">训练效果</h6>
<p>准确率 &amp;&amp; 损失</p>
<p>迭代次数较小的准确率并不高，但存在"突飞猛进"点，即epoch为2-4区间发生了猛增，并保持高准确率，虽前期准确率没有上面两种结构高，但学习速度远超于以上两种模型，此外，损失率也很快逼近0并保持，不存在过大起伏，基本保持持续下降后保持</p>
<p><img src="/post/cda691ed/DeepCNN_accuracies.jpg" alt="DeepCNN_accuracies" style="zoom:20%;"><img src="/post/cda691ed/DeepCNN_losses.jpg" alt="DeepCNN_losses" style="zoom:20%;"></p>
<h6 id="fgsm攻击效果-2">FGSM攻击效果</h6>
<p>样本展示</p>
<p><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.05_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.05_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.1_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.1_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.15_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.15_batch_0_image_1" style="zoom:33%;"></p>
<p><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.2_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.2_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.25_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.25_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/DeepCNN_fgsm_epsilon_0.3_batch_0_image_1.png" alt="DeepCNN_fgsm_epsilon_0.3_batch_0_image_1" style="zoom:33%;"></p>
<p>攻击后的准确率随着设置的epsilons变化</p>
<p>效果略好于SimpleCNN，但优势不明显，eps为0.1-0.15变化最快</p>
<p><img src="/post/cda691ed/DeepCNN_fgsm_accuracies.jpg" alt="DeepCNN_fgsm_accuracies" style="zoom:25%;"></p>
<h6 id="对抗训练之后的攻击效果-2"><strong>对抗训练之后的攻击效果</strong></h6>
<p>当以eps=0.1的对抗样本对模型做对抗训练，同样以eps为0.1的对抗样本做测试，训练过程准确率虽有上升但不稳定，右边是对抗不同eps产生的对抗样本的效果，比上图有改进，但效果不大，对eps再增加产生的对抗样本的对抗效果不好</p>
<p><img src="/post/cda691ed/DeepCNN_adversarial_fgsm_accuracies.jpg" alt="DeepCNN_adversarial_fgsm_accuracies" style="zoom: 20%;"><img src="/post/cda691ed/DeepCNN_adversarial_accuracies.jpg" alt="DeepCNN_adversarial_accuracies" style="zoom:20%;"></p>
<h6 id="pgd攻击效果-2">PGD攻击效果</h6>
<p>自变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eps：epsilon，对抗攻击的幅度，即每次攻击时允许的最大扰动量。</span><br><span class="line">num_iter:轮数，对抗攻击迭代的次数，即进行对抗攻击的循环次数。</span><br><span class="line">alpha：单步攻击步长，每次对原始样本进行梯度上升的步长大小</span><br></pre></td></tr></table></figure>
<p>eps大于0.15变化趋势和上面两种一致，且对抗效果优于以上两种神经网络结构，同样存在eps为0.05时，其他参数增加反而降低攻击效果</p>
<p><img src="/post/cda691ed/DeepCNN_0.05_accuracies.jpg" alt="DeepCNN_0.05_accuracies" style="zoom:10%;"><img src="/post/cda691ed/DeepCNN_0.1_accuracies.jpg" alt="DeepCNN_0.1_accuracies" style="zoom: 10%;"><img src="/post/cda691ed/DeepCNN_0.15_accuracies.jpg" alt="DeepCNN_0.15_accuracies" style="zoom:10%;"></p>
<p><img src="/post/cda691ed/DeepCNN_0.2_accuracies.jpg" alt="DeepCNN_0.2_accuracies" style="zoom:10%;"><img src="/post/cda691ed/DeepCNN_0.25_accuracies.jpg" alt="DeepCNN_0.25_accuracies" style="zoom:10%;"><img src="/post/cda691ed/DeepCNN_0.3_accuracies.jpg" alt="DeepCNN_0.3_accuracies" style="zoom:10%;"></p>
<h5 id="残差神经网络">残差神经网络</h5>
<p>模型结构</p>
<figure>
<img src="/post/cda691ed/ResNet.png" alt="ResNet">
<figcaption aria-hidden="true">ResNet</figcaption>
</figure>
<h6 id="训练效果-3">训练效果</h6>
<p>准确率 &amp;&amp; 损失</p>
<p>效果时这四种模型中效果最好的，即使迭代次数少，但准确率已能达到98.7%以上，甚至超过了上面某些模型迭代十次的效果，损失率页很快就逼进了0</p>
<p><img src="/post/cda691ed/ResNet18_accuracies.jpg" alt="ResNet18_accuracies" style="zoom:20%;"><img src="/post/cda691ed/ResNet18_losses.jpg" alt="ResNet18_losses" style="zoom:20%;"></p>
<h6 id="fgsm攻击效果-3">FGSM攻击效果</h6>
<p>样本展示</p>
<p><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.05_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.05_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.1_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.1_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.15_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.15_batch_0_image_1" style="zoom:33%;"></p>
<p><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.2_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.2_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.25_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.25_batch_0_image_1" style="zoom:33%;"><img src="/post/cda691ed/ResNet18_fgsm_epsilon_0.3_batch_0_image_1.png" alt="ResNet18_fgsm_epsilon_0.3_batch_0_image_1" style="zoom:33%;"></p>
<p>攻击后的准确率随着设置的epsilons变化</p>
<p>对抗效果虽略优于其他模型，但是准确率仍下降到了0.2以下，只是没有特别逼近0，按照后面的趋势，应该能保证这个幅度</p>
<p><img src="/post/cda691ed/ResNet18_fgsm_accuracies.jpg" alt="ResNet18_fgsm_accuracies" style="zoom:25%;"></p>
<h6 id="对抗训练之后的攻击效果-3"><strong>对抗训练之后的攻击效果</strong></h6>
<p>当以eps=0.1的对抗样本对模型做对抗训练，同样以eps为0.1的对抗样本做测试，和深度卷积网络一样，训练过程没有保持持续上升，但是准确率提升很多，右边是对抗不同eps产生的对抗样本的效果，相比于没有对抗训练的效果提升很多</p>
<p><img src="/post/cda691ed/ResNet18_adversarial_fgsm_accuracies.jpg" alt="ResNet18_adversarial_fgsm_accuracies" style="zoom:20%;"><img src="/post/cda691ed/ResNet18_adversarial_accuracies.jpg" alt="ResNet18_adversarial_accuracies" style="zoom:20%;"></p>
<h6 id="pgd攻击效果-3">PGD攻击效果</h6>
<p>自变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eps：epsilon，对抗攻击的幅度，即每次攻击时允许的最大扰动量。</span><br><span class="line">num_iter:轮数，对抗攻击迭代的次数，即进行对抗攻击的循环次数。</span><br><span class="line">alpha：单步攻击步长，每次对原始样本进行梯度上升的步长大小</span><br></pre></td></tr></table></figure>
<p>对抗效果是最差的，eps从0.1开始，其准确率就下降到了10%以下</p>
<p><img src="/post/cda691ed/ResNet18_0.05_accuracies.jpg" alt="ResNet18_0.05_accuracies" style="zoom:10%;"><img src="/post/cda691ed/ResNet18_0.1_accuracies.jpg" alt="ResNet18_0.1_accuracies" style="zoom:10%;"><img src="/post/cda691ed/ResNet18_0.15_accuracies.jpg" alt="ResNet18_0.15_accuracies" style="zoom:10%;"></p>
<p><img src="/post/cda691ed/ResNet18_0.2_accuracies.jpg" alt="ResNet18_0.2_accuracies" style="zoom:10%;"><img src="/post/cda691ed/ResNet18_0.25_accuracies.jpg" alt="ResNet18_0.25_accuracies" style="zoom:10%;"><img src="/post/cda691ed/ResNet18_0.3_accuracies.jpg" alt="ResNet18_0.3_accuracies" style="zoom:10%;"></p>
<h4 id="分析总结">分析总结</h4>
<h6 id="某些情况中当alpha和num_iter增大时攻击效果反而变差">某些情况中，当alpha和num_iter增大时，攻击效果反而变差</h6>
<p><strong>过度迭代导致过拟合对抗方向</strong>：</p>
<ul>
<li>如果步长和迭代次数太大，扰动可能会变得过于复杂（特别eps较小时），远离了使得模型误分类的有效方向，从而导致对抗样本变得无效。</li>
<li>过度迭代可能会导致对抗样本变得更加离散，不再沿着最有效的攻击方向。</li>
</ul>
<p><strong>数值稳定性问题</strong>：</p>
<ul>
<li>过大的步长和迭代次数可能会导致数值不稳定性，扰动值可能超出有效范围，无法正确引导模型误分类。</li>
</ul>
<h6 id="resnet在非对抗情况下表现最好但在对抗攻击下表现最差">ResNet在非对抗情况下表现最好，但在对抗攻击下表现最差</h6>
<p><strong>模型复杂性和过拟合</strong>：</p>
<ul>
<li>ResNet由于其复杂性和深度，在训练时可能会过度拟合训练数据，导致在对抗攻击时，模型更容易受到影响。</li>
<li>复杂的模型可能捕捉到数据中的细微特征，导致对抗样本能更容易地找到这些特征的脆弱点。</li>
</ul>
<h6 id="epsilonε">epsilon（ε）</h6>
<p>在PGD攻击中，epsilon（ε）代表了对抗扰动的最大幅度，alpha（α）是每一步的步长，而num_iter是总的迭代次数。固定epsilon之后，alpha和num_iter的作用相对较小：</p>
<p><strong>epsilon定义了扰动的上限</strong>：</p>
<ul>
<li>epsilon（ε）直接控制了扰动的最大幅度。因此，无论alpha和num_iter如何调整，扰动的总幅度不能超过epsilon定义的范围。这意味着epsilon是决定攻击强度的主要因素。</li>
</ul>
<p><strong>alpha和num_iter影响攻击的细节</strong>：</p>
<ul>
<li>alpha和num_iter更多地影响了生成对抗样本的路径和细节。当epsilon固定时，增加alpha和num_iter可以帮助更细致地探索对抗方向，但最终的扰动幅度还是受到epsilon的限制。</li>
<li>增大num_iter会使攻击更加逼近epsilon所能达到的极限，而增大alpha则可能会让每一步的扰动更大，但由于最终扰动不能超过epsilon，过大的alpha可能导致不稳定的扰动，反而不利于有效攻击。</li>
</ul>
<p><strong>步长和迭代次数的权衡</strong>：</p>
<ul>
<li>较大的alpha可能在每一步产生过大的扰动，导致无法有效逼近最优对抗方向。而较小的alpha配合更多的迭代次数可以在epsilon范围内更细致地调整扰动，使得攻击效果更佳。</li>
<li>num_iter的增加可以让扰动更充分地逼近epsilon的边界，但如果alpha过大，num_iter的增加作用会减弱，因为每一步的扰动已经很接近epsilon。</li>
</ul>
<h6 id="对抗训练有效提升模型对抗样本的鲁棒性">对抗训练有效提升模型对抗样本的鲁棒性</h6>
<p>通过对抗训练，模型（特别是ResNet）的准确率在对抗样本上有显著提升。这表明对抗训练有效增强了模型对对抗扰动的抵抗能力。对抗训练通过在训练过程中引入对抗样本，使模型学会如何处理和抵御这些样本，从而在测试时能够更好地应对对抗攻击。</p>
<h5 id="总结-1">总结</h5>
<ol type="1">
<li><strong>epsilon是决定攻击强度的主要因素</strong>：
<ul>
<li>它定义了对抗扰动的最大范围，因此是决定攻击效果的主要因素。</li>
</ul></li>
<li><strong>alpha和num_iter的选择需要平衡</strong>：
<ul>
<li>适中的alpha和num_iter组合能够在epsilon范围内生成有效的对抗扰动。过大的alpha和num_iter可能导致扰动过度复杂化，扰动效果反而下降。</li>
</ul></li>
<li><strong>复杂模型对抗效果的特殊性</strong>：
<ul>
<li>复杂模型（如ResNet）在高维特征空间中的扰动路径更容易不稳定，因此需要特别注意alpha和num_iter的选择。</li>
</ul></li>
<li><strong>对抗训练的重要性</strong>
<ul>
<li>对抗训练增强了模型的泛化能力，使其在面对未见过的对抗样本时也能表现出色</li>
</ul></li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>FGSM &amp; PDG</p><p><a href="http://example.com/post/cda691ed.html">http://example.com/post/cda691ed.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>yyyyyyxnp</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-07-02</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-09-29</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/post/374aa791.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">leetcode-daily-practice</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/post/eb02829d.html"><span class="level-item">JNDI学习</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Yyyyyyxnp"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Yyyyyyxnp</p><p class="is-size-6 is-block">Student Yang</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Sichuan University</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">48</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/deng12yx" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/deng12yx"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#fgsm"><span class="level-left"><span class="level-item">1</span><span class="level-item">FGSM</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#研究点"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">研究点</span></span></a></li><li><a class="level is-mobile" href="#线性模型中的对抗扰动"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">线性模型中的对抗扰动</span></span></a></li><li><a class="level is-mobile" href="#非线性模型的线性特性"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">非线性模型的线性特性</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#lstm长短期记忆网络"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">LSTM（长短期记忆网络）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#公式和结构"><span class="level-left"><span class="level-item">1.3.1.1</span><span class="level-item">公式和结构：</span></span></a></li><li><a class="level is-mobile" href="#线性特性"><span class="level-left"><span class="level-item">1.3.1.2</span><span class="level-item">线性特性：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#relu整流线性单元"><span class="level-left"><span class="level-item">1.3.2</span><span class="level-item">ReLU（整流线性单元）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#公式和特性"><span class="level-left"><span class="level-item">1.3.2.1</span><span class="level-item">公式和特性：</span></span></a></li><li><a class="level is-mobile" href="#线性特性-1"><span class="level-left"><span class="level-item">1.3.2.2</span><span class="level-item">线性特性：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#maxout网络"><span class="level-left"><span class="level-item">1.3.3</span><span class="level-item">Maxout网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#公式和特性-1"><span class="level-left"><span class="level-item">1.3.3.1</span><span class="level-item">公式和特性：</span></span></a></li><li><a class="level is-mobile" href="#线性特性-2"><span class="level-left"><span class="level-item">1.3.3.2</span><span class="level-item">线性特性：</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#初始fgsm"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">初始FGSM</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#具体实例"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">具体实例</span></span></a></li></ul></li><li><a class="level is-mobile" href="#l1正则化-vs-fgsm对抗性训练"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">L1正则化 vs FGSM对抗性训练</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#l1正则化"><span class="level-left"><span class="level-item">1.5.1</span><span class="level-item">L1正则化</span></span></a></li><li><a class="level is-mobile" href="#对抗性训练"><span class="level-left"><span class="level-item">1.5.2</span><span class="level-item">对抗性训练</span></span></a></li><li><a class="level is-mobile" href="#总结"><span class="level-left"><span class="level-item">1.5.3</span><span class="level-item">总结</span></span></a></li></ul></li><li><a class="level is-mobile" href="#深度模型的对抗扰动"><span class="level-left"><span class="level-item">1.6</span><span class="level-item">深度模型的对抗扰动</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#对抗性训练-vs-数据增强"><span class="level-left"><span class="level-item">1.6.1</span><span class="level-item">对抗性训练 vs 数据增强</span></span></a></li><li><a class="level is-mobile" href="#损失函数优化"><span class="level-left"><span class="level-item">1.6.2</span><span class="level-item">损失函数优化</span></span></a></li><li><a class="level is-mobile" href="#噪声添加位置选择"><span class="level-left"><span class="level-item">1.6.3</span><span class="level-item">噪声添加位置选择</span></span></a></li><li><a class="level is-mobile" href="#容量低的模型表现出好性能rbf"><span class="level-left"><span class="level-item">1.6.4</span><span class="level-item">容量低的模型表现出好性能（RBF）</span></span></a></li><li><a class="level is-mobile" href="#对抗样本在不同模型之间的泛化现象"><span class="level-left"><span class="level-item">1.6.5</span><span class="level-item">对抗样本在不同模型之间的泛化现象</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#pgd"><span class="level-left"><span class="level-item">2</span><span class="level-item">PGD</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#研究点-1"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">研究点</span></span></a></li><li><a class="level is-mobile" href="#pgd方法"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">PGD方法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#原理总结"><span class="level-left"><span class="level-item">3</span><span class="level-item">原理总结</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#快速梯度符号方法fgsm"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">1. 快速梯度符号方法（FGSM）：</span></span></a></li><li><a class="level is-mobile" href="#投影梯度下降pgd"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">2. 投影梯度下降（PGD）：</span></span></a></li><li><a class="level is-mobile" href="#区别总结"><span class="level-left"><span class="level-item">3.1.3</span><span class="level-item">区别总结：</span></span></a></li></ul></ul></li><li><a class="level is-mobile" href="#实验部分"><span class="level-left"><span class="level-item">4</span><span class="level-item">实验部分</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#实验环境和基本参数"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">实验环境和基本参数</span></span></a></li><li><a class="level is-mobile" href="#模型选择"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">模型选择</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#简单全连接神经网络"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">简单全连接神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#训练效果"><span class="level-left"><span class="level-item">4.2.1.1</span><span class="level-item">训练效果</span></span></a></li><li><a class="level is-mobile" href="#fgsm攻击效果"><span class="level-left"><span class="level-item">4.2.1.2</span><span class="level-item">FGSM攻击效果</span></span></a></li><li><a class="level is-mobile" href="#对抗训练之后的攻击效果"><span class="level-left"><span class="level-item">4.2.1.3</span><span class="level-item">对抗训练之后的攻击效果</span></span></a></li><li><a class="level is-mobile" href="#pgd攻击效果"><span class="level-left"><span class="level-item">4.2.1.4</span><span class="level-item">PGD攻击效果</span></span></a></li></ul></li><li><a class="level is-mobile" href="#卷积神经网络"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">卷积神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#训练效果-1"><span class="level-left"><span class="level-item">4.2.2.1</span><span class="level-item">训练效果</span></span></a></li><li><a class="level is-mobile" href="#fgsm攻击效果-1"><span class="level-left"><span class="level-item">4.2.2.2</span><span class="level-item">FGSM攻击效果</span></span></a></li><li><a class="level is-mobile" href="#对抗训练之后的攻击效果-1"><span class="level-left"><span class="level-item">4.2.2.3</span><span class="level-item">对抗训练之后的攻击效果</span></span></a></li><li><a class="level is-mobile" href="#pgd攻击效果-1"><span class="level-left"><span class="level-item">4.2.2.4</span><span class="level-item">PGD攻击效果</span></span></a></li></ul></li><li><a class="level is-mobile" href="#深度卷积神经网络"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">深度卷积神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#训练效果-2"><span class="level-left"><span class="level-item">4.2.3.1</span><span class="level-item">训练效果</span></span></a></li><li><a class="level is-mobile" href="#fgsm攻击效果-2"><span class="level-left"><span class="level-item">4.2.3.2</span><span class="level-item">FGSM攻击效果</span></span></a></li><li><a class="level is-mobile" href="#对抗训练之后的攻击效果-2"><span class="level-left"><span class="level-item">4.2.3.3</span><span class="level-item">对抗训练之后的攻击效果</span></span></a></li><li><a class="level is-mobile" href="#pgd攻击效果-2"><span class="level-left"><span class="level-item">4.2.3.4</span><span class="level-item">PGD攻击效果</span></span></a></li></ul></li><li><a class="level is-mobile" href="#残差神经网络"><span class="level-left"><span class="level-item">4.2.4</span><span class="level-item">残差神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#训练效果-3"><span class="level-left"><span class="level-item">4.2.4.1</span><span class="level-item">训练效果</span></span></a></li><li><a class="level is-mobile" href="#fgsm攻击效果-3"><span class="level-left"><span class="level-item">4.2.4.2</span><span class="level-item">FGSM攻击效果</span></span></a></li><li><a class="level is-mobile" href="#对抗训练之后的攻击效果-3"><span class="level-left"><span class="level-item">4.2.4.3</span><span class="level-item">对抗训练之后的攻击效果</span></span></a></li><li><a class="level is-mobile" href="#pgd攻击效果-3"><span class="level-left"><span class="level-item">4.2.4.4</span><span class="level-item">PGD攻击效果</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#分析总结"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">分析总结</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#某些情况中当alpha和num_iter增大时攻击效果反而变差"><span class="level-left"><span class="level-item">4.3.1.1</span><span class="level-item">某些情况中，当alpha和num_iter增大时，攻击效果反而变差</span></span></a></li><li><a class="level is-mobile" href="#resnet在非对抗情况下表现最好但在对抗攻击下表现最差"><span class="level-left"><span class="level-item">4.3.1.2</span><span class="level-item">ResNet在非对抗情况下表现最好，但在对抗攻击下表现最差</span></span></a></li><li><a class="level is-mobile" href="#epsilonε"><span class="level-left"><span class="level-item">4.3.1.3</span><span class="level-item">epsilon（ε）</span></span></a></li><li><a class="level is-mobile" href="#对抗训练有效提升模型对抗样本的鲁棒性"><span class="level-left"><span class="level-item">4.3.1.4</span><span class="level-item">对抗训练有效提升模型对抗样本的鲁棒性</span></span></a></li></ul><li><a class="level is-mobile" href="#总结-1"><span class="level-left"><span class="level-item">4.3.2</span><span class="level-item">总结</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-09-01T13:27:46.000Z">2025-09-01</time></p><p class="title"><a href="/post/faa1c95e.html">ctfshow初学</a></p><p class="categories"><a href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/post/b64de532.html"><img src="/img/article/23.png" alt="ai_tools"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-25T05:34:16.000Z">2024-12-25</time></p><p class="title"><a href="/post/b64de532.html">ai_tools</a></p><p class="categories"><a href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/post/d75064c8.html"><img src="/img/article/18.png" alt="基础知识学习"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-10-09T11:54:53.000Z">2024-10-09</time></p><p class="title"><a href="/post/d75064c8.html">基础知识学习</a></p><p class="categories"><a href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/post/9c1a1a1.html"><img src="/img/article/17.png" alt="luogu_1"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-10-08T10:50:27.000Z">2024-10-08</time></p><p class="title"><a href="/post/9c1a1a1.html">luogu_1</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/post/7684f23b.html"><img src="/img/article/15.png" alt="T2I"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-09-23T06:17:27.000Z">2024-09-23</time></p><p class="title"><a href="/post/7684f23b.html">T2I</a></p><p class="categories"><a href="/categories/%E5%AE%89%E5%85%A8/">安全</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">BeautyFlower</a><p class="is-size-7"><span>&copy; 2025 yyyyyyxnp</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/deng12yx"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>